{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc312f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c68e7a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7c21eb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text='Lemmatization is the process of converting a word to its base form. The difference between stemming and lemmatization is, lemmatization considers the context and converts the word to its meaningful base form, whereas stemming just removes the last few characters, often leading to incorrect meanings and spelling errors.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a6cb968b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Lemmatization is the process of converting a word to its base form.', 'The difference between stemming and lemmatization is, lemmatization considers the context and converts the word to its meaningful base form, whereas stemming just removes the last few characters, often leading to incorrect meanings and spelling errors.']\n"
     ]
    }
   ],
   "source": [
    "tokens_sents = nltk.sent_tokenize(text)\n",
    "print(tokens_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "206d537b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Lemmatization', 'is', 'the', 'process', 'of', 'converting', 'a', 'word', 'to', 'its', 'base', 'form', '.', 'The', 'difference', 'between', 'stemming', 'and', 'lemmatization', 'is', ',', 'lemmatization', 'considers', 'the', 'context', 'and', 'converts', 'the', 'word', 'to', 'its', 'meaningful', 'base', 'form', ',', 'whereas', 'stemming', 'just', 'removes', 'the', 'last', 'few', 'characters', ',', 'often', 'leading', 'to', 'incorrect', 'meanings', 'and', 'spelling', 'errors', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens_words = nltk.word_tokenize(text)\n",
    "print(tokens_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b9f969f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0c510535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lemmat', 'is', 'the', 'process', 'of', 'convert', 'a', 'word', 'to', 'it', 'base', 'form', '.', 'the', 'differ', 'between', 'stem', 'and', 'lemmat', 'is', ',', 'lemmat', 'consid', 'the', 'context', 'and', 'convert', 'the', 'word', 'to', 'it', 'meaning', 'base', 'form', ',', 'wherea', 'stem', 'just', 'remov', 'the', 'last', 'few', 'charact', ',', 'often', 'lead', 'to', 'incorrect', 'mean', 'and', 'spell', 'error', '.']\n"
     ]
    }
   ],
   "source": [
    "stem=[]\n",
    "for i in tokens_words:\n",
    "  ps = PorterStemmer()\n",
    "  stem_word= ps.stem(i)\n",
    "  stem.append(stem_word)\n",
    "print(stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d75e7c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1be743cf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lemmatized_output' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [25]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# lemmatized_output = ' '.join([lemmatizer.lemmatize(w) for w in stem])\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mlemmatized_output\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'lemmatized_output' is not defined"
     ]
    }
   ],
   "source": [
    "# lemmatized_output = ' '.join([lemmatizer.lemmatize(w) for w in stem])\n",
    "print(lemmatized_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dcb85fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lemmat', 'is', 'the', 'process', 'of', 'convert', 'a', 'word', 'to', 'it', 'base', 'form', '.', 'the', 'differ', 'between', 'stem', 'and', 'lemmat', 'is', ',', 'lemmat', 'consid', 'the', 'context', 'and', 'convert', 'the', 'word', 'to', 'it', 'meaning', 'base', 'form', ',', 'wherea', 'stem', 'just', 'remov', 'the', 'last', 'few', 'charact', ',', 'often', 'lead', 'to', 'incorrect', 'mean', 'and', 'spell', 'error', '.']\n"
     ]
    }
   ],
   "source": [
    "leme=[]\n",
    "for i in stem:\n",
    "  lemetized_word=lemmatizer.lemmatize(i)\n",
    "  leme.append(lemetized_word)\n",
    "print(leme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5441d3e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parts of Speech:  [('lemmat', 'NN'), ('is', 'VBZ'), ('the', 'DT'), ('process', 'NN'), ('of', 'IN'), ('convert', 'NN'), ('a', 'DT'), ('word', 'NN'), ('to', 'TO'), ('it', 'PRP'), ('base', 'JJ'), ('form', 'NN'), ('.', '.'), ('the', 'DT'), ('differ', 'NN'), ('between', 'IN'), ('stem', 'NN'), ('and', 'CC'), ('lemmat', 'NN'), ('is', 'VBZ'), (',', ','), ('lemmat', 'JJ'), ('consid', 'VBP'), ('the', 'DT'), ('context', 'NN'), ('and', 'CC'), ('convert', 'VB'), ('the', 'DT'), ('word', 'NN'), ('to', 'TO'), ('it', 'PRP'), ('meaning', 'VBG'), ('base', 'JJ'), ('form', 'NN'), (',', ','), ('wherea', 'JJ'), ('stem', 'NN'), ('just', 'RB'), ('remov', 'VB'), ('the', 'DT'), ('last', 'JJ'), ('few', 'JJ'), ('charact', 'NN'), (',', ','), ('often', 'RB'), ('lead', 'VBP'), ('to', 'TO'), ('incorrect', 'VB'), ('mean', 'NN'), ('and', 'CC'), ('spell', 'NN'), ('error', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "print(\"Parts of Speech: \",nltk.pos_tag(leme))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "03a68747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "sw_nltk = stopwords.words('english')\n",
    "print(sw_nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e14665e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization process converting word base form. difference stemming lemmatization is, lemmatization considers context converts word meaningful base form, whereas stemming removes last characters, often leading incorrect meanings spelling errors.\n"
     ]
    }
   ],
   "source": [
    "words = [word for word in text.split() if word.lower() not in sw_nltk]\n",
    "new_text = \" \".join(words)\n",
    "print(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5b0e9d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Lemmatization', 'is', 'the', 'process', 'of', 'converting', 'a', 'word', 'to', 'its', 'base', 'form', '.', 'The', 'difference', 'between', 'stemming', 'and', 'lemmatization', 'is', ',', 'lemmatization', 'considers', 'the', 'context', 'and', 'converts', 'the', 'word', 'to', 'its', 'meaningful', 'base', 'form', ',', 'whereas', 'stemming', 'just', 'removes', 'the', 'last', 'few', 'characters', ',', 'often', 'leading', 'to', 'incorrect', 'meanings', 'and', 'spelling', 'errors', '.']\n",
      "['Lemmatization', 'process', 'converting', 'word', 'base', 'form', '.', 'The', 'difference', 'stemming', 'lemmatization', ',', 'lemmatization', 'considers', 'context', 'converts', 'word', 'meaningful', 'base', 'form', ',', 'whereas', 'stemming', 'removes', 'last', 'characters', ',', 'often', 'leading', 'incorrect', 'meanings', 'spelling', 'errors', '.']\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "  \n",
    "word_tokens = word_tokenize(text)\n",
    "# converts the words in word_tokens to lower case and then checks whether \n",
    "#they are present in stop_words or not\n",
    "filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "#with no lower case conversion\n",
    "filtered_sentence = []\n",
    "  \n",
    "for w in word_tokens:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "  \n",
    "print(word_tokens)\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5c5c2499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization  :  lemmat\n",
      "is  :  is\n",
      "the  :  the\n",
      "process  :  process\n",
      "of  :  of\n",
      "converting  :  convert\n",
      "a  :  a\n",
      "word  :  word\n",
      "to  :  to\n",
      "its  :  it\n",
      "base  :  base\n",
      "form  :  form\n",
      ".  :  .\n",
      "The  :  the\n",
      "difference  :  differ\n",
      "between  :  between\n",
      "stemming  :  stem\n",
      "and  :  and\n",
      "lemmatization  :  lemmat\n",
      "is  :  is\n",
      ",  :  ,\n",
      "lemmatization  :  lemmat\n",
      "considers  :  consid\n",
      "the  :  the\n",
      "context  :  context\n",
      "and  :  and\n",
      "converts  :  convert\n",
      "the  :  the\n",
      "word  :  word\n",
      "to  :  to\n",
      "its  :  it\n",
      "meaningful  :  meaning\n",
      "base  :  base\n",
      "form  :  form\n",
      ",  :  ,\n",
      "whereas  :  wherea\n",
      "stemming  :  stem\n",
      "just  :  just\n",
      "removes  :  remov\n",
      "the  :  the\n",
      "last  :  last\n",
      "few  :  few\n",
      "characters  :  charact\n",
      ",  :  ,\n",
      "often  :  often\n",
      "leading  :  lead\n",
      "to  :  to\n",
      "incorrect  :  incorrect\n",
      "meanings  :  mean\n",
      "and  :  and\n",
      "spelling  :  spell\n",
      "errors  :  error\n",
      ".  :  .\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    " \n",
    "ps = PorterStemmer()\n",
    " \n",
    "words = word_tokenize(text)\n",
    " \n",
    "for w in words:\n",
    "    print(w, \" : \", ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "881dd13e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['benzema', 'dor', 'madrid', 'real', 'runner', 'salah', 'season', 'set', 'ucl', 'win']\n",
      "(1, 11)\n",
      "[[0.26726124 0.26726124 0.26726124 0.26726124 0.26726124 0.26726124\n",
      "  0.26726124 0.26726124 0.26726124 0.26726124 0.53452248]]\n"
     ]
    }
   ],
   "source": [
    "sentence=['Real madrid is set to win the UCL for the season . Benzema might win Balon dor . Salah might be the runner up']\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n",
    "\n",
    "TFIDF = vectorizer.fit_transform(sentence)\n",
    "print(vectorizer.get_feature_names()[-10:])\n",
    "print(TFIDF.shape)\n",
    "print(TFIDF.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33186c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
